# MM147 Record
The system test has not been completed yet, but I'm leading by about 8 points in second place, so I'm likely to win.  


## Final Approach
One of the top solution was to estimate a function by simulated annealing. However, when considering score maximization,  should not only estimate one function, but one should consider a set of all reasonable functions and output a grid from which the expected value of the score can be maximized.   

This can be done by taking the average of the results obtained after multi-start simulated annealing, but there is a limitation because the number of trials is reduced.   

Therefore, I estimated it using UNet.   
UNet was trained with the input as a nearest neighbor interpolation of the query result and a bitmap of the location where the query was called.   
Generated by calling the query `randint(N^2/150,N^2/50)` times at random positions on a grid generated in the same way as the tester, which I used as input for training.   

There are inputs from 20\*20 to 60\*60, but since I want to make cell-by-cell predictions, we train in all 8 increments from 24\*24 to 64\*64, and if 60\*60, I use a 64\*64 model and estimate with UNet, assuming 4 cells outside without resizing.


The next step is to determine the position of the queries, which should be evenly spaced apart from each other. This could easily be done by using the positions of the grid points, but this can only be accomplished if the number of queries is a square number. Therefore, by embedding the result of circle packing in a square, the number of queries can be adjusted in increments of 1.
reference:   [http://www.packomania.com/](http://www.packomania.com/)

We then determined the number of queries from N,A given by the input.
This allowed me to reach 80 points.  

## Details
Each improves the score by about 1-2%.  

- If the correct value is known from the query results, use the value, and fine-tune around it as well.
- Scale the results of circle packing in a square around the origin and move it a little closer to the wall.
- Order the queries with `y+x` in decreasing order, and if the variance of sequence of differences is below a threshold when half of the queries are completed, randomly remove a few vertices from those that have not yet completed the query, and fine-tune the position of the remaining vertices by hill climbing.
- If it is estimated that E is small and the function consist only apply_xy2, then a model trained only in that case is used (determined by whether there is a 3*3 area where all values are less than 2)
- Adjust parameters by 1K cases


## Other Comments
- Because of the nature of xor, when a cliff exists at position x, a cliff is likely to occur at x+8 and x+16 as well, so I combined fully-connected layers in some parts of UNet to create vertical and horizontal stripes, but it did not improve the situation.
- I thought that calling the extremes with a query would improve the estimation results, but it was better to take them regularly.


Test Case #1: Score = 16883.499422940342  
Test Case #2: Score = 2213.004557291666  
Test Case #3: Score = 8667.548543700072  
Test Case #4: Score = 5403.129705452702  
Test Case #5: Score = 4921.028625234523  
Test Case #6: Score = 2755.8822784132  
Test Case #7: Score = 10608.297326799131  
Test Case #8: Score = 11504.110505003266  
Test Case #9: Score = 3184.182869039126  
Test Case #10: Score = 4270.83517363291  